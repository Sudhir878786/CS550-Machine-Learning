{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 550 Homework 1 .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Su9ljeqCf3vI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** **Sudhir Sharma**\n",
        "\n",
        "**ID NO** **12041500**\n",
        "\n",
        "**Branch** **CSE**\n",
        "\n",
        "**CS 550 Homework 1**"
      ],
      "metadata": {
        "id": "C14NaPv9xBbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART A **  \n",
        "ans  \n",
        "** a)** \n",
        "As the pricing equation is given as : P = C*max {L, 20} + 100  \n",
        "                                 Where staring value of c is 20  and L : Length of the journey (kms)\n",
        "          Using The given equation the minimum price of a ticket using the starting values (ie 20) comes       \n",
        "          out to be 500.\n"
      ],
      "metadata": {
        "id": "ftt81fXEbfGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) ** \n",
        "\n",
        "If user accept's the ride then value of C will increase by 0.01*[max(l,20)] to max. the profit and if its rejected then C will decrease by 0.01*[max(l,20)].\n",
        "\n",
        "In the event that there is demand, the update rule tries to capitalise on it, and it makes up for poor demand by lowering the price proportionate to the existing price.\n",
        "Let's change it this way. \n",
        "\n",
        "if the user accepts the rides at the quoted price:\n",
        "\n",
        "      C = C + scaling factor âˆ—âˆ‡cP/P\n",
        "\n",
        "if the user rejects the ride at the quoted price\n",
        "\n",
        "      C = max(C âˆ’ scaling factor âˆ—âˆ‡cP/P,threshold)\n",
        "\n",
        " Although the scaling factor is set here to 0.01 as its initial value, it can be adjusted to any number to match the beginning value. The lowest value of C that the firm can afford is called the threshold. With this improvement, the spike rate is considerably lower than it was before. \n"
      ],
      "metadata": {
        "id": "XfwNR0SzbzdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c**)\n"
      ],
      "metadata": {
        "id": "UJxCaQOGbdjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "C = 5\n",
        "train = [\n",
        "    {'l':10, 'Accepted':'YES'},\n",
        "    {'l':20, 'Accepted':'YES'},\n",
        "    {'l':10, 'Accepted':'YES'},\n",
        "    {'l':15, 'Accepted':'NO'},\n",
        "    {'l':35, 'Accepted':'YES'},\n",
        "    {'l':30, 'Accepted':'NO'},\n",
        "]"
      ],
      "metadata": {
        "id": "rXK56MfZc3lz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train:\n",
        "   l = i['l']\n",
        "   v = i['Accepted']\n",
        "   print(\"C before :\",C)\n",
        "   print(\"Price :\",C*max(l,20) + 100)\n",
        "   if v == \"YES\":\n",
        "      C = C + 0.01*max(l, 20)\n",
        "   else:\n",
        "      C = C - 0.01*max(l, 20)\n",
        "   print(\"Updated value Of C: \",C)\n",
        "   print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-Z1X2W5keFjv",
        "outputId": "78c9f5bd-78d2-4e0a-867f-4f731c035c02"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C before : 5\n",
            "Price : 200\n",
            "Updated value Of C:  5.2\n",
            "\n",
            "C before : 5.2\n",
            "Price : 204.0\n",
            "Updated value Of C:  5.4\n",
            "\n",
            "C before : 5.4\n",
            "Price : 208.0\n",
            "Updated value Of C:  5.6000000000000005\n",
            "\n",
            "C before : 5.6000000000000005\n",
            "Price : 212.0\n",
            "Updated value Of C:  5.4\n",
            "\n",
            "C before : 5.4\n",
            "Price : 289.0\n",
            "Updated value Of C:  5.75\n",
            "\n",
            "C before : 5.75\n",
            "Price : 272.5\n",
            "Updated value Of C:  5.45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "          \n",
        "L \tPrice Quoted \tUser Purchases? \t      C \n",
        "10 \t200          \tYes \t                  5.2\n",
        "20 \t204         \tYes \t                  5.4\n",
        "10 \t208         \tYes                  \t  5.6\n",
        "15 \t212         \tNo \t                    5.4\n",
        "35 \t289         \tYes         \t          5.75\n",
        "30\t272         \tNo\t                    5.45\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "srZT9XKHtPpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART B**\n",
        "\n",
        "**I** a. The training set is for model training and the test set is for model validation.\n",
        "\n",
        "**Explaination**\n",
        "\n",
        "The reason is that when the dataset is split into train and test sets, there will not be enough data in the training dataset for the model to learn an effective mapping of inputs to outputs. There will also not be enough data in the test set to effectively evaluate the model performance.\n",
        "So, We train the model firstly ,then ascertain if its working properly on the test set.\n",
        "\n",
        "**II** c. Traditional computing problems such as inverting matrices, multiplying numbers etc\n",
        "\n",
        "\n",
        "**Explaination**\n",
        "\n",
        "Traditional issues can't be greatly enhanced using Machine Learning.\n",
        "\n",
        "**III**  a. one training sample\n",
        "\n",
        "\n",
        "**Explaination**\n",
        "\n",
        "Stochastic gradient method uses one sample at each iteration, giving it faster convergence and less accuracy.\n",
        "\n",
        "An important parameter of Gradient Descent (GD) is the size of the steps, determined by the learning rate hyperparameters. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time, and if it is too high we may jump the optimal value.\n",
        "\n",
        "**IV**\n",
        "Third image depicts underfitting.\n",
        "\n",
        "**V** Gradient Descent works well for large number of features, because it uses O(n) time complexity.\n",
        "\n",
        "We can also use batch gradient descent, stochastic gradient descent, or mini-batch gradient descent. SGD and MBGD would work the best because neither of them need to load the entire dataset into memory in order to take 1 step of gradient descent. Batch would be ok with the caveat that you have enough memory to load all the data.\n",
        "\n",
        "The normal equations method would not be a good choice because it is computationally inefficient. The main cause of the computational complexity comes from inverse operation on an (n x n) matrix.\n",
        "\n",
        "O n2 . 4 to O n3\n",
        "\n",
        "**VI** Gradient Descent algorithms suffer most by outliers, since they are overfitting by default. We can apply feature\n",
        "scaling to converge quickly.\n",
        "\n",
        "**VII** Gradient descent produces a convex shaped graph which only has one global optimum. Therefore, it cannot get stuck in a local minimum.\n",
        "\n",
        "**VIII**\n",
        "When the training error and validation error are close to each other and high that means your model is underfitting (i.e. it has high bias). You should try to reduce the regularization hyperparameter.\n",
        "\n",
        "**IX** No. The issue is that stochastic gradient descent and mini-batch gradient descent have randomness built into them. This means that they can find their way to nearby the global optimum, but they generally don't converge. One way to help them converge is to gradually reduce the learning rate hyperparameter.\n",
        "\n",
        "**X** When there are many features and few of them can be dropped because of their low influence on target. It simplifies\n",
        "our model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0rTzPfIKimQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART C**\n",
        "\n",
        "The distance function Iâ€™m using is euclidean distance\n",
        "\n",
        "distance = math.sqrt(((x2 - x1) ** 2) + (y2 - y1) ** 2)"
      ],
      "metadata": {
        "id": "95vmL91MfmCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    {\"label\": \"Normal\", \"image\": np.array([[2, 1, 2], [1, 2, 1], [2, 1, 1]])},\n",
        "    {\"label\": \"A\", \"image\": np.array([[4, 3, 2], [3, 3, 1], [2, 1, 1]])},\n",
        "    {\"label\": \"B\", \"image\": np.array([[2, 1, 2], [1, 2, 3], [2, 3, 4]])},\n",
        "    {\"label\": \"C\", \"image\": np.array([[2, 3, 4], [1, 3, 3], [2, 1, 1]])},\n",
        "]"
      ],
      "metadata": {
        "id": "E0An8r-Jf6ji"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\n",
        "    np.array([\n",
        "        [1, 2, 2],\n",
        "        [1, 2, 1],\n",
        "        [1, 1, 1]\n",
        "    ]),\n",
        "    np.array([\n",
        "        [1, 4, 4],\n",
        "        [1, 3, 4],\n",
        "        [2, 2, 1]\n",
        "    ])\n",
        "]"
      ],
      "metadata": {
        "id": "msTGVgUBf-Hy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After calculating for each test \n",
        "\n",
        "distance(Normal,t1)= 3\n",
        "\n",
        "distance(A,t1)= 16 \n",
        "\n",
        "distance(B,t1)= 20 \n",
        "\n",
        "distance(C,t1)= 12 \n",
        "\n",
        "distance(Normal,t2)= 25 \n",
        "\n",
        "distance(A,t2)= 28 \n",
        "\n",
        "distance(B,t2)= 26 \n",
        "\n",
        "distance(C,t2)= 4\n",
        "\n",
        "So t1 is normal and t2 is C.\n"
      ],
      "metadata": {
        "id": "j41GNug8hrEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART D**\n",
        "\n",
        "$$\n",
        "w^{(t+1)} = w^t + Î·_{t}âˆ‘_{n=1}^{N}2(y_n-w^{t^T}x_n)x_n\n",
        "$$\n",
        "\n",
        " Assuming N=1, GD will be \n",
        "\n",
        "$$\n",
        "w^{(t+1)}=w^t + Î·_{t}2(y-w^{t^T}x)x\n",
        "$$\n",
        "Taking Transpose  both side \n",
        "\n",
        "$$\n",
        " w^{(t+1)^T}x=w^{t^T}x+Î·_{t}2(y-x^Tw^t)x^T\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "   y-w^{(t+1)^T}x=(y-w^{t^T}x)-Î·_{t}2(y-x^Tw^t)x^Tx\n",
        "$$\n",
        " as \n",
        "$$\n",
        "   x^Tx>=0.x^Tw^t=w{t^T}x\n",
        "$$\n",
        "\n",
        "if $$ ((y-w^{t^T}x)== is positive $$  \n",
        "\n",
        "$$\n",
        " y-w^{{t+1}^T}x<((y-w^{t^T}x)-(+ve)\n",
        "$$\n",
        "else if $$((y-w^{t^T}x) ==is negative $$\n",
        "$$ y-w^{{t+1}^T}x=((y-w^{t^T}x)-(-ve)$$\n",
        "\n",
        "$$ |y-w^{{t+1}^T}x|<((y-w^{t^T}x) $$\n",
        "\n",
        "<!-- $$\n",
        "w^{t^T}x_n - w^{(t+1)^T}x_n\n",
        "$$\n",
        "\n",
        "$$\n",
        "[w^{t^T} - w^{t^T} - Î·_t2(y_n - w^{t^T}x_n)x_n^T]x_n \\tag{1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "[- Î·_t2(y_n - w^{t^T} x_n)x_n^T]x_n\n",
        "$$  -->\n",
        "So \n",
        "\n",
        "Assuming  ğ‘ =1,\n",
        "\n",
        "Hence shown that GD update improves prediction on the training input (ğ’™ğ‘›, ğ‘¦ğ‘›), i.e, ğ‘¦ğ‘› is \n",
        "closer to ğ’˜(ğ‘¡+1)âŠ¤ğ’™ğ‘› than to ğ’˜(ğ‘¡)âŠ¤ğ’™ğ‘› "
      ],
      "metadata": {
        "id": "IuUrfBpqmh45"
      }
    }
  ]
}